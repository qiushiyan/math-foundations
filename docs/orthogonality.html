<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 3 Orthogonality | Mathematical Notes for Machine Learning</title>
  <meta name="description" content="Chapter 3 Orthogonality | Mathematical Notes for Machine Learning" />
  <meta name="generator" content="bookdown 0.20 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 3 Orthogonality | Mathematical Notes for Machine Learning" />
  <meta property="og:type" content="book" />
  
  
  
  <meta name="github-repo" content="enixam/math-foundations" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 3 Orthogonality | Mathematical Notes for Machine Learning" />
  
  
  

<meta name="author" content="Qiushi Yan" />


<meta name="date" content="2020-09-18" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="vector-spaces.html"/>
<link rel="next" href="eigenthings-and-quadratic-forms.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>



<link rel="stylesheet" href="css\style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Math Notes for Machine Learning</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="part"><span><b>I Linear Algebra</b></span></li>
<li class="chapter" data-level="1" data-path="basic-matrix-algebra.html"><a href="basic-matrix-algebra.html"><i class="fa fa-check"></i><b>1</b> Basic Matrix Algebra</a><ul>
<li class="chapter" data-level="1.1" data-path="basic-matrix-algebra.html"><a href="basic-matrix-algebra.html#matrix-multiplication"><i class="fa fa-check"></i><b>1.1</b> Matrix Multiplication</a><ul>
<li class="chapter" data-level="1.1.1" data-path="basic-matrix-algebra.html"><a href="basic-matrix-algebra.html#geometric-transformations"><i class="fa fa-check"></i><b>1.1.1</b> Geometric Transformations</a></li>
<li class="chapter" data-level="1.1.2" data-path="basic-matrix-algebra.html"><a href="basic-matrix-algebra.html#matrix-multiplication-as-linear-transformation"><i class="fa fa-check"></i><b>1.1.2</b> Matrix Multiplication as Linear Transformation</a></li>
<li class="chapter" data-level="1.1.3" data-path="basic-matrix-algebra.html"><a href="basic-matrix-algebra.html#selector-matrix"><i class="fa fa-check"></i><b>1.1.3</b> Selector Matrix</a></li>
<li class="chapter" data-level="1.1.4" data-path="basic-matrix-algebra.html"><a href="basic-matrix-algebra.html#discrete-convolution"><i class="fa fa-check"></i><b>1.1.4</b> Discrete Convolution</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="basic-matrix-algebra.html"><a href="basic-matrix-algebra.html#elemetary-matrix-and-row-operations"><i class="fa fa-check"></i><b>1.2</b> Elemetary matrix and row operations</a></li>
<li class="chapter" data-level="1.3" data-path="basic-matrix-algebra.html"><a href="basic-matrix-algebra.html#lu-factorization"><i class="fa fa-check"></i><b>1.3</b> LU Factorization</a></li>
<li class="chapter" data-level="1.4" data-path="basic-matrix-algebra.html"><a href="basic-matrix-algebra.html#determinants"><i class="fa fa-check"></i><b>1.4</b> Determinants</a><ul>
<li class="chapter" data-level="1.4.1" data-path="basic-matrix-algebra.html"><a href="basic-matrix-algebra.html#cofactor-expansion"><i class="fa fa-check"></i><b>1.4.1</b> Cofactor Expansion</a></li>
<li class="chapter" data-level="1.4.2" data-path="basic-matrix-algebra.html"><a href="basic-matrix-algebra.html#geometric-interpretation-of-determinant"><i class="fa fa-check"></i><b>1.4.2</b> Geometric Interpretation of Determinant</a></li>
<li class="chapter" data-level="1.4.3" data-path="basic-matrix-algebra.html"><a href="basic-matrix-algebra.html#properties-of-determinant"><i class="fa fa-check"></i><b>1.4.3</b> Properties of Determinant</a></li>
<li class="chapter" data-level="1.4.4" data-path="basic-matrix-algebra.html"><a href="basic-matrix-algebra.html#cramers-rule"><i class="fa fa-check"></i><b>1.4.4</b> Cramer’s Rule</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="basic-matrix-algebra.html"><a href="basic-matrix-algebra.html#trace"><i class="fa fa-check"></i><b>1.5</b> Trace</a></li>
<li class="chapter" data-level="1.6" data-path="basic-matrix-algebra.html"><a href="basic-matrix-algebra.html#matrix-inversion"><i class="fa fa-check"></i><b>1.6</b> Matrix Inversion</a><ul>
<li class="chapter" data-level="1.6.1" data-path="basic-matrix-algebra.html"><a href="basic-matrix-algebra.html#the-matrix-inversion-lemma"><i class="fa fa-check"></i><b>1.6.1</b> The Matrix Inversion Lemma</a></li>
</ul></li>
<li class="chapter" data-level="1.7" data-path="basic-matrix-algebra.html"><a href="basic-matrix-algebra.html#complexity-of-matrix-computation"><i class="fa fa-check"></i><b>1.7</b> Complexity of Matrix Computation</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="vector-spaces.html"><a href="vector-spaces.html"><i class="fa fa-check"></i><b>2</b> Vector Spaces</a><ul>
<li class="chapter" data-level="2.1" data-path="vector-spaces.html"><a href="vector-spaces.html#vector-space"><i class="fa fa-check"></i><b>2.1</b> Vector Space</a><ul>
<li class="chapter" data-level="2.1.1" data-path="vector-spaces.html"><a href="vector-spaces.html#euclidean-space"><i class="fa fa-check"></i><b>2.1.1</b> Euclidean Space</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="vector-spaces.html"><a href="vector-spaces.html#metric-spaces-normed-spaces-inner-product-spaces"><i class="fa fa-check"></i><b>2.2</b> Metric Spaces, Normed Spaces, Inner Product Spaces</a><ul>
<li class="chapter" data-level="2.2.1" data-path="vector-spaces.html"><a href="vector-spaces.html#metric-and-norm"><i class="fa fa-check"></i><b>2.2.1</b> Metric and Norm</a></li>
<li class="chapter" data-level="2.2.2" data-path="vector-spaces.html"><a href="vector-spaces.html#inner-produc-outer-product-cross-product"><i class="fa fa-check"></i><b>2.2.2</b> Inner Produc, Outer Product, Cross Product</a></li>
<li class="chapter" data-level="2.2.3" data-path="vector-spaces.html"><a href="vector-spaces.html#restricted-definition-of-inner-products-in-rn"><i class="fa fa-check"></i><b>2.2.3</b> Restricted Definition of Inner Products in <span class="math inline">\(R^n\)</span></a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="vector-spaces.html"><a href="vector-spaces.html#subspaces"><i class="fa fa-check"></i><b>2.3</b> Subspaces</a></li>
<li class="chapter" data-level="2.4" data-path="vector-spaces.html"><a href="vector-spaces.html#fundamental-theorem"><i class="fa fa-check"></i><b>2.4</b> Fundamental Theorem of Linear Algebra</a></li>
<li class="chapter" data-level="2.5" data-path="vector-spaces.html"><a href="vector-spaces.html#rank"><i class="fa fa-check"></i><b>2.5</b> Rank</a><ul>
<li class="chapter" data-level="2.5.1" data-path="vector-spaces.html"><a href="vector-spaces.html#effect-of-operations-on-matrix-rank"><i class="fa fa-check"></i><b>2.5.1</b> Effect of Operations on Matrix Rank</a></li>
<li class="chapter" data-level="2.5.2" data-path="vector-spaces.html"><a href="vector-spaces.html#gram-matrix"><i class="fa fa-check"></i><b>2.5.2</b> Gram Matrix</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="vector-spaces.html"><a href="vector-spaces.html#bases-and-coordinate-systems"><i class="fa fa-check"></i><b>2.6</b> Bases and Coordinate Systems</a><ul>
<li class="chapter" data-level="2.6.1" data-path="vector-spaces.html"><a href="vector-spaces.html#change-of-basis"><i class="fa fa-check"></i><b>2.6.1</b> Change of Basis</a></li>
</ul></li>
<li class="chapter" data-level="2.7" data-path="vector-spaces.html"><a href="vector-spaces.html#complexity-of-vector-computations"><i class="fa fa-check"></i><b>2.7</b> Complexity of Vector Computations</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="orthogonality.html"><a href="orthogonality.html"><i class="fa fa-check"></i><b>3</b> Orthogonality</a><ul>
<li class="chapter" data-level="3.1" data-path="orthogonality.html"><a href="orthogonality.html#orthogonal-decomposition"><i class="fa fa-check"></i><b>3.1</b> Orthogonal Decomposition</a><ul>
<li class="chapter" data-level="3.1.1" data-path="orthogonality.html"><a href="orthogonality.html#orthogonal-complements"><i class="fa fa-check"></i><b>3.1.1</b> Orthogonal Complements</a></li>
<li class="chapter" data-level="3.1.2" data-path="orthogonality.html"><a href="orthogonality.html#orthogonal-sets-and-orthogonal-basis"><i class="fa fa-check"></i><b>3.1.2</b> Orthogonal Sets and Orthogonal Basis</a></li>
<li class="chapter" data-level="3.1.3" data-path="orthogonality.html"><a href="orthogonality.html#orthogonal-decomposition-1"><i class="fa fa-check"></i><b>3.1.3</b> Orthogonal Decomposition</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="orthogonality.html"><a href="orthogonality.html#orthonormal-sets-and-orthogonal-matrices"><i class="fa fa-check"></i><b>3.2</b> Orthonormal Sets and Orthogonal Matrices</a><ul>
<li class="chapter" data-level="3.2.1" data-path="orthogonality.html"><a href="orthogonality.html#orthogonal-matrices"><i class="fa fa-check"></i><b>3.2.1</b> Orthogonal Matrices</a></li>
<li class="chapter" data-level="3.2.2" data-path="orthogonality.html"><a href="orthogonality.html#best-approximation"><i class="fa fa-check"></i><b>3.2.2</b> Best Approximation</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="orthogonality.html"><a href="orthogonality.html#projection-and-idempotent-matrices"><i class="fa fa-check"></i><b>3.3</b> Projection and idempotent matrices</a></li>
<li class="chapter" data-level="3.4" data-path="orthogonality.html"><a href="orthogonality.html#gram-schmidt-process"><i class="fa fa-check"></i><b>3.4</b> Gram-Schmidt Process</a></li>
<li class="chapter" data-level="3.5" data-path="orthogonality.html"><a href="orthogonality.html#qr-factorizaiton"><i class="fa fa-check"></i><b>3.5</b> QR Factorizaiton</a></li>
<li class="chapter" data-level="3.6" data-path="orthogonality.html"><a href="orthogonality.html#complexity"><i class="fa fa-check"></i><b>3.6</b> Complexity</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="eigenthings-and-quadratic-forms.html"><a href="eigenthings-and-quadratic-forms.html"><i class="fa fa-check"></i><b>4</b> Eigenthings and Quadratic Forms</a><ul>
<li class="chapter" data-level="4.1" data-path="eigenthings-and-quadratic-forms.html"><a href="eigenthings-and-quadratic-forms.html#eigenvectors-and-eigenvalues"><i class="fa fa-check"></i><b>4.1</b> Eigenvectors and Eigenvalues</a><ul>
<li class="chapter" data-level="4.1.1" data-path="eigenthings-and-quadratic-forms.html"><a href="eigenthings-and-quadratic-forms.html#additional-properties-of-eigenvalues-and-eigenvectors"><i class="fa fa-check"></i><b>4.1.1</b> Additional Properties of Eigenvalues and Eigenvectors</a></li>
<li class="chapter" data-level="4.1.2" data-path="eigenthings-and-quadratic-forms.html"><a href="eigenthings-and-quadratic-forms.html#left-eigenvectors-and-right-eigenvectors"><i class="fa fa-check"></i><b>4.1.2</b> Left Eigenvectors and Right Eigenvectors</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="eigenthings-and-quadratic-forms.html"><a href="eigenthings-and-quadratic-forms.html#diagnolization-and-similar-matrices"><i class="fa fa-check"></i><b>4.2</b> Diagnolization and Similar Matrices</a><ul>
<li class="chapter" data-level="4.2.1" data-path="eigenthings-and-quadratic-forms.html"><a href="eigenthings-and-quadratic-forms.html#similarity"><i class="fa fa-check"></i><b>4.2.1</b> Similarity</a></li>
<li class="chapter" data-level="4.2.2" data-path="eigenthings-and-quadratic-forms.html"><a href="eigenthings-and-quadratic-forms.html#jordan-matrix"><i class="fa fa-check"></i><b>4.2.2</b> Jordan Matrix</a></li>
<li class="chapter" data-level="4.2.3" data-path="eigenthings-and-quadratic-forms.html"><a href="eigenthings-and-quadratic-forms.html#simultaneous-diagonalization"><i class="fa fa-check"></i><b>4.2.3</b> Simultaneous Diagonalization</a></li>
<li class="chapter" data-level="4.2.4" data-path="eigenthings-and-quadratic-forms.html"><a href="eigenthings-and-quadratic-forms.html#cayley-hamilton-theorem"><i class="fa fa-check"></i><b>4.2.4</b> Cayley-Hamilton Theorem</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="eigenthings-and-quadratic-forms.html"><a href="eigenthings-and-quadratic-forms.html#symmetric-matrices"><i class="fa fa-check"></i><b>4.3</b> Symmetric Matrices</a><ul>
<li class="chapter" data-level="4.3.1" data-path="eigenthings-and-quadratic-forms.html"><a href="eigenthings-and-quadratic-forms.html#spectral-decomposition"><i class="fa fa-check"></i><b>4.3.1</b> Spectral Decomposition</a></li>
<li class="chapter" data-level="4.3.2" data-path="eigenthings-and-quadratic-forms.html"><a href="eigenthings-and-quadratic-forms.html#a-orthogonality"><i class="fa fa-check"></i><b>4.3.2</b> A-Orthogonality</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="eigenthings-and-quadratic-forms.html"><a href="eigenthings-and-quadratic-forms.html#quadratic-forms"><i class="fa fa-check"></i><b>4.4</b> Quadratic Forms</a><ul>
<li class="chapter" data-level="4.4.1" data-path="eigenthings-and-quadratic-forms.html"><a href="eigenthings-and-quadratic-forms.html#change-of-variable"><i class="fa fa-check"></i><b>4.4.1</b> Change of Variable</a></li>
<li class="chapter" data-level="4.4.2" data-path="eigenthings-and-quadratic-forms.html"><a href="eigenthings-and-quadratic-forms.html#classification-of-quadratic-forms"><i class="fa fa-check"></i><b>4.4.2</b> Classification of Quadratic Forms</a></li>
<li class="chapter" data-level="4.4.3" data-path="eigenthings-and-quadratic-forms.html"><a href="eigenthings-and-quadratic-forms.html#gershgorin-discs-and-diagonal-dominance"><i class="fa fa-check"></i><b>4.4.3</b> Gershgorin Discs and Diagonal Dominance</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="eigenthings-and-quadratic-forms.html"><a href="eigenthings-and-quadratic-forms.html#cholesky-factorization"><i class="fa fa-check"></i><b>4.5</b> Cholesky Factorization</a></li>
<li class="chapter" data-level="4.6" data-path="eigenthings-and-quadratic-forms.html"><a href="eigenthings-and-quadratic-forms.html#rayleigh-quotients"><i class="fa fa-check"></i><b>4.6</b> Rayleigh Quotients</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="singular-value-decomposition.html"><a href="singular-value-decomposition.html"><i class="fa fa-check"></i><b>5</b> Singular Value Decomposition</a><ul>
<li class="chapter" data-level="5.1" data-path="singular-value-decomposition.html"><a href="singular-value-decomposition.html#singular-values"><i class="fa fa-check"></i><b>5.1</b> Singular Values</a></li>
<li class="chapter" data-level="5.2" data-path="singular-value-decomposition.html"><a href="singular-value-decomposition.html#svd-theorem"><i class="fa fa-check"></i><b>5.2</b> SVD</a></li>
<li class="chapter" data-level="5.3" data-path="singular-value-decomposition.html"><a href="singular-value-decomposition.html#matrix-norms"><i class="fa fa-check"></i><b>5.3</b> Matrix Norms</a><ul>
<li class="chapter" data-level="5.3.1" data-path="singular-value-decomposition.html"><a href="singular-value-decomposition.html#induced-norms"><i class="fa fa-check"></i><b>5.3.1</b> Induced Norms</a></li>
<li class="chapter" data-level="5.3.2" data-path="singular-value-decomposition.html"><a href="singular-value-decomposition.html#entry-wise-norm"><i class="fa fa-check"></i><b>5.3.2</b> Entry-wise Norm</a></li>
<li class="chapter" data-level="5.3.3" data-path="singular-value-decomposition.html"><a href="singular-value-decomposition.html#other-matrix-norms"><i class="fa fa-check"></i><b>5.3.3</b> Other Matrix Norms</a></li>
<li class="chapter" data-level="5.3.4" data-path="singular-value-decomposition.html"><a href="singular-value-decomposition.html#unitary-invariant-norms"><i class="fa fa-check"></i><b>5.3.4</b> Unitary Invariant Norms</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="singular-value-decomposition.html"><a href="singular-value-decomposition.html#low-rank-approximation"><i class="fa fa-check"></i><b>5.4</b> Low Rank Approximation</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="linear-system.html"><a href="linear-system.html"><i class="fa fa-check"></i><b>6</b> Solutions of Linear System Ax = b</a><ul>
<li class="chapter" data-level="6.1" data-path="linear-system.html"><a href="linear-system.html#lesat-squares-problems"><i class="fa fa-check"></i><b>6.1</b> Lesat Squares Problems</a></li>
<li class="chapter" data-level="6.2" data-path="linear-system.html"><a href="linear-system.html#generalized-inverse"><i class="fa fa-check"></i><b>6.2</b> Generalized Inverse</a><ul>
<li class="chapter" data-level="6.2.1" data-path="linear-system.html"><a href="linear-system.html#left-and-right-inverse"><i class="fa fa-check"></i><b>6.2.1</b> Left and Right Inverse</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="linear-system.html"><a href="linear-system.html#ill-conditioned-matrices"><i class="fa fa-check"></i><b>6.3</b> Ill-Conditioned Matrices</a><ul>
<li class="chapter" data-level="6.3.1" data-path="linear-system.html"><a href="linear-system.html#condition-number"><i class="fa fa-check"></i><b>6.3.1</b> Condition Number</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>II Multivariate Calculus</b></span></li>
<li class="chapter" data-level="7" data-path="partial-derivatives.html"><a href="partial-derivatives.html"><i class="fa fa-check"></i><b>7</b> Partial Derivatives</a><ul>
<li class="chapter" data-level="7.1" data-path="partial-derivatives.html"><a href="partial-derivatives.html#limit-and-continuity"><i class="fa fa-check"></i><b>7.1</b> Limit and Continuity</a></li>
<li class="chapter" data-level="7.2" data-path="partial-derivatives.html"><a href="partial-derivatives.html#partial-derivative"><i class="fa fa-check"></i><b>7.2</b> Partial Derivative</a><ul>
<li class="chapter" data-level="7.2.1" data-path="partial-derivatives.html"><a href="partial-derivatives.html#gradient-and-directional-derivative"><i class="fa fa-check"></i><b>7.2.1</b> Gradient and Directional Derivative</a></li>
<li class="chapter" data-level="7.2.2" data-path="partial-derivatives.html"><a href="partial-derivatives.html#linearization-of-two-variable-functions"><i class="fa fa-check"></i><b>7.2.2</b> Linearization of Two-variable Functions</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="partial-derivatives.html"><a href="partial-derivatives.html#differentials"><i class="fa fa-check"></i><b>7.3</b> Differentials</a><ul>
<li class="chapter" data-level="7.3.1" data-path="partial-derivatives.html"><a href="partial-derivatives.html#continuity-partial-derivatives-and-differentiability"><i class="fa fa-check"></i><b>7.3.1</b> Continuity, Partial Derivatives and Differentiability</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="partial-derivatives.html"><a href="partial-derivatives.html#divergence-curl-and-laplacian"><i class="fa fa-check"></i><b>7.4</b> Divergence, Curl, and Laplacian</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="matrix-calculus.html"><a href="matrix-calculus.html"><i class="fa fa-check"></i><b>8</b> Matrix Calculus</a><ul>
<li class="chapter" data-level="8.1" data-path="matrix-calculus.html"><a href="matrix-calculus.html#the-chain-rule"><i class="fa fa-check"></i><b>8.1</b> The Chain Rule</a></li>
<li class="chapter" data-level="8.2" data-path="matrix-calculus.html"><a href="matrix-calculus.html#useful-identities-in-matirx-calculus"><i class="fa fa-check"></i><b>8.2</b> Useful Identities in Matirx Calculus</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="taylor-series.html"><a href="taylor-series.html"><i class="fa fa-check"></i><b>9</b> Taylor Series</a><ul>
<li class="chapter" data-level="9.1" data-path="taylor-series.html"><a href="taylor-series.html#convergence-of-taylor-series"><i class="fa fa-check"></i><b>9.1</b> Convergence of Taylor Series</a></li>
<li class="chapter" data-level="9.2" data-path="taylor-series.html"><a href="taylor-series.html#taylor-approximation-of-multivariate-functions"><i class="fa fa-check"></i><b>9.2</b> Taylor Approximation of Multivariate Functions</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="multiple-integral.html"><a href="multiple-integral.html"><i class="fa fa-check"></i><b>10</b> Multiple Integral</a></li>
<li class="part"><span><b>III Probability Theory</b></span></li>
<li class="chapter" data-level="11" data-path="basics-of-probability-theory.html"><a href="basics-of-probability-theory.html"><i class="fa fa-check"></i><b>11</b> Basics of Probability Theory</a><ul>
<li class="chapter" data-level="11.1" data-path="basics-of-probability-theory.html"><a href="basics-of-probability-theory.html#probabilty-space"><i class="fa fa-check"></i><b>11.1</b> Probabilty Space</a></li>
<li class="chapter" data-level="11.2" data-path="basics-of-probability-theory.html"><a href="basics-of-probability-theory.html#counting"><i class="fa fa-check"></i><b>11.2</b> Counting</a></li>
<li class="chapter" data-level="11.3" data-path="basics-of-probability-theory.html"><a href="basics-of-probability-theory.html#conditional-probability"><i class="fa fa-check"></i><b>11.3</b> Conditional Probability</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="random-variables-and-moments.html"><a href="random-variables-and-moments.html"><i class="fa fa-check"></i><b>12</b> Random variables and moments</a><ul>
<li class="chapter" data-level="12.1" data-path="random-variables-and-moments.html"><a href="random-variables-and-moments.html#properties-of-expectation-and-variance"><i class="fa fa-check"></i><b>12.1</b> Properties of Expectation and Variance</a><ul>
<li class="chapter" data-level="12.1.1" data-path="random-variables-and-moments.html"><a href="random-variables-and-moments.html#random-vectors"><i class="fa fa-check"></i><b>12.1.1</b> Random vectors</a></li>
</ul></li>
<li class="chapter" data-level="12.2" data-path="random-variables-and-moments.html"><a href="random-variables-and-moments.html#other-summaries"><i class="fa fa-check"></i><b>12.2</b> Other Summaries</a></li>
<li class="chapter" data-level="12.3" data-path="random-variables-and-moments.html"><a href="random-variables-and-moments.html#moment-generating-functions"><i class="fa fa-check"></i><b>12.3</b> Moment Generating Functions</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="univariate-distributions.html"><a href="univariate-distributions.html"><i class="fa fa-check"></i><b>13</b> Univariate Distributions</a><ul>
<li class="chapter" data-level="13.1" data-path="univariate-distributions.html"><a href="univariate-distributions.html#uniform-distribution"><i class="fa fa-check"></i><b>13.1</b> Uniform Distribution</a></li>
<li class="chapter" data-level="13.2" data-path="univariate-distributions.html"><a href="univariate-distributions.html#normal-distribution"><i class="fa fa-check"></i><b>13.2</b> Normal Distribution</a><ul>
<li class="chapter" data-level="13.2.1" data-path="univariate-distributions.html"><a href="univariate-distributions.html#log-normal-distribution"><i class="fa fa-check"></i><b>13.2.1</b> Log Normal Distribution</a></li>
</ul></li>
<li class="chapter" data-level="13.3" data-path="univariate-distributions.html"><a href="univariate-distributions.html#binomial-distribution-and-beta-distribution"><i class="fa fa-check"></i><b>13.3</b> Binomial Distribution and Beta Distribution</a></li>
<li class="chapter" data-level="13.4" data-path="univariate-distributions.html"><a href="univariate-distributions.html#poisson-distribution"><i class="fa fa-check"></i><b>13.4</b> Poisson Distribution</a><ul>
<li class="chapter" data-level="13.4.1" data-path="univariate-distributions.html"><a href="univariate-distributions.html#poisson-process"><i class="fa fa-check"></i><b>13.4.1</b> Poisson Process</a></li>
</ul></li>
<li class="chapter" data-level="13.5" data-path="univariate-distributions.html"><a href="univariate-distributions.html#exponential-distribution-and-gamma-distribution"><i class="fa fa-check"></i><b>13.5</b> Exponential Distribution and Gamma Distribution</a><ul>
<li class="chapter" data-level="13.5.1" data-path="univariate-distributions.html"><a href="univariate-distributions.html#properties"><i class="fa fa-check"></i><b>13.5.1</b> Properties</a></li>
<li class="chapter" data-level="13.5.2" data-path="univariate-distributions.html"><a href="univariate-distributions.html#inverse-gamma-distribution"><i class="fa fa-check"></i><b>13.5.2</b> Inverse Gamma Distribution</a></li>
</ul></li>
<li class="chapter" data-level="13.6" data-path="univariate-distributions.html"><a href="univariate-distributions.html#beta-distribution"><i class="fa fa-check"></i><b>13.6</b> Beta Distribution</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html"><i class="fa fa-check"></i><b>14</b> Multivariate Distributions</a><ul>
<li class="chapter" data-level="14.1" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#multivariate-normal-distribution"><i class="fa fa-check"></i><b>14.1</b> Multivariate Normal Distribution</a><ul>
<li class="chapter" data-level="14.1.1" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#chi-square-distribution"><i class="fa fa-check"></i><b>14.1.1</b> Chi-square Distribution</a></li>
</ul></li>
<li class="chapter" data-level="14.2" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#dirichlet-distributon"><i class="fa fa-check"></i><b>14.2</b> Dirichlet Distributon</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="markov-chain.html"><a href="markov-chain.html"><i class="fa fa-check"></i><b>15</b> Markov Chain</a></li>
<li class="part"><span><b>IV Learning Theory</b></span></li>
<li class="chapter" data-level="16" data-path="the-learning-problem-framework.html"><a href="the-learning-problem-framework.html"><i class="fa fa-check"></i><b>16</b> The Learning Problem Framework</a><ul>
<li class="chapter" data-level="16.1" data-path="the-learning-problem-framework.html"><a href="the-learning-problem-framework.html#the-pac-learning-framework"><i class="fa fa-check"></i><b>16.1</b> The PAC Learning Framework</a></li>
</ul></li>
<li class="part"><span><b>V Optimization</b></span></li>
<li class="chapter" data-level="17" data-path="basics-of-optimization.html"><a href="basics-of-optimization.html"><i class="fa fa-check"></i><b>17</b> Basics of Optimization</a><ul>
<li class="chapter" data-level="17.1" data-path="basics-of-optimization.html"><a href="basics-of-optimization.html#univariate-optimization"><i class="fa fa-check"></i><b>17.1</b> Univariate Optimization</a></li>
<li class="chapter" data-level="17.2" data-path="basics-of-optimization.html"><a href="basics-of-optimization.html#multivariate-optimization"><i class="fa fa-check"></i><b>17.2</b> Multivariate Optimization</a></li>
<li class="chapter" data-level="17.3" data-path="basics-of-optimization.html"><a href="basics-of-optimization.html#convex-functions"><i class="fa fa-check"></i><b>17.3</b> Convex Functions</a></li>
<li class="chapter" data-level="17.4" data-path="basics-of-optimization.html"><a href="basics-of-optimization.html#method-of-lagrange-multiplier"><i class="fa fa-check"></i><b>17.4</b> Method of Lagrange Multiplier</a></li>
</ul></li>
<li class="chapter" data-level="18" data-path="gradient-descent.html"><a href="gradient-descent.html"><i class="fa fa-check"></i><b>18</b> Gradient Descent</a></li>
<li class="part"><span><b>VI Applications</b></span></li>
<li class="chapter" data-level="19" data-path="linear-models.html"><a href="linear-models.html"><i class="fa fa-check"></i><b>19</b> Linear Models</a><ul>
<li class="chapter" data-level="19.1" data-path="linear-models.html"><a href="linear-models.html#ordinary-least-squares"><i class="fa fa-check"></i><b>19.1</b> Ordinary Least Squares</a><ul>
<li class="chapter" data-level="19.1.1" data-path="linear-models.html"><a href="linear-models.html#least-square-estimation"><i class="fa fa-check"></i><b>19.1.1</b> Least Square Estimation</a></li>
</ul></li>
<li class="chapter" data-level="19.2" data-path="linear-models.html"><a href="linear-models.html#weighted-least-squares"><i class="fa fa-check"></i><b>19.2</b> Weighted Least Squares</a></li>
<li class="chapter" data-level="19.3" data-path="linear-models.html"><a href="linear-models.html#partial-least-squares"><i class="fa fa-check"></i><b>19.3</b> Partial Least Squares</a></li>
<li class="chapter" data-level="19.4" data-path="linear-models.html"><a href="linear-models.html#regularized-regression"><i class="fa fa-check"></i><b>19.4</b> Regularized Regression</a><ul>
<li class="chapter" data-level="19.4.1" data-path="linear-models.html"><a href="linear-models.html#ridge-regression"><i class="fa fa-check"></i><b>19.4.1</b> Ridge Regression</a></li>
<li class="chapter" data-level="19.4.2" data-path="linear-models.html"><a href="linear-models.html#lasso"><i class="fa fa-check"></i><b>19.4.2</b> Lasso</a></li>
<li class="chapter" data-level="19.4.3" data-path="linear-models.html"><a href="linear-models.html#elastic-net"><i class="fa fa-check"></i><b>19.4.3</b> Elastic Net</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="20" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html"><i class="fa fa-check"></i><b>20</b> Principal Component Analysis</a></li>
<li class="chapter" data-level="21" data-path="text-mining.html"><a href="text-mining.html"><i class="fa fa-check"></i><b>21</b> Text Mining</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li>
  <a href="https://github.com/rstudio/bookdown" target="blank">Written with bookdown</a>
</li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Mathematical Notes for Machine Learning</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="orthogonality" class="section level1">
<h1><span class="header-section-number">Chapter 3</span> Orthogonality</h1>
<div id="orthogonal-decomposition" class="section level2">
<h2><span class="header-section-number">3.1</span> Orthogonal Decomposition</h2>
<div id="orthogonal-complements" class="section level3">
<h3><span class="header-section-number">3.1.1</span> Orthogonal Complements</h3>
<p>if vector <span class="math inline">\(\bar{v}\)</span> is orthogonal to every vector in a subspace <span class="math inline">\(W\)</span> of <span class="math inline">\(\mathbb{R^n}\)</span>, then <span class="math inline">\(\bar{v}\)</span> is said to be orthogonal to <span class="math inline">\(W\)</span>. The subspace that contains the set of vectors that are orthogonal to <span class="math inline">\(W\)</span> is called the <strong>orthogonal complement</strong>, denoted by <span class="math inline">\(W^{\perp}\)</span>.</p>
<p><span class="math display">\[
W^{\perp} = \{\bar{v} \in W^{\perp} | \;\bar{v} \perp \bar{x} \; \text{for all} \; \bar{x} \in W\}
\]</span></p>
<p>This corresponds to discussions in Section <a href="vector-spaces.html#fundamental-theorem">2.4</a>, where</p>
<p><span class="math display">\[
\mathcal{R}(A^T) = \mathcal{N}(A) \\
\mathcal{R}(A) = \mathcal{N}{(A^T)}
\]</span>
<br></p>

<div class="theorem">
<span id="thm:unnamed-chunk-1" class="theorem"><strong>Theorem 1.2  </strong></span>If <span class="math inline">\(W\)</span> is a subspace of <span class="math inline">\(\mathbb{R}^n\)</span>, <span class="math inline">\(W^{\perp}\)</span> is also a subspace of <span class="math inline">\(\mathbb{R}^n\)</span>.
</div>

<p>It’s easy to verify that <span class="math inline">\(W^{\perp}\)</span> is closed under scalar multiplication, and under vector addition, and that any vector in <span class="math inline">\(W\)</span> has <span class="math inline">\(n\)</span> components. So that <span class="math inline">\(W^{\perp}\)</span> is a subspace of <span class="math inline">\(\mathbb{R}^n\)</span></p>
</div>
<div id="orthogonal-sets-and-orthogonal-basis" class="section level3">
<h3><span class="header-section-number">3.1.2</span> Orthogonal Sets and Orthogonal Basis</h3>
<p>An orthogonal set is a set of vectors
<span class="math inline">\(\{\bar{u}_1, \dots, \bar{u}_p\}\)</span> in <span class="math inline">\(\mathbb{R^n}\)</span>, in which each pair of distinct vectors is orthogonal: <span class="math inline">\(\bar{u}_i^{T} \bar{u}_j = 0 \quad i\not = j\)</span>. Note that the set do not necessarily span the whole <span class="math inline">\(\mathbb{R^n}\)</span>, but a subspace <span class="math inline">\(W\)</span>.</p>
<p>Since vectors in orthogonal sets is mutually perpendicular, they must also be linearly independent and could form a basis for a subspace <span class="math inline">\(W\)</span>. In such case, they are called <strong>orthogonal basis</strong>.</p>
<p>There is a particular advantage in using orthogonal basis rather than other basis, because we can find a easy representation of any vector in <span class="math inline">\(W\)</span>.</p>

<div class="theorem">
<p><span id="thm:unnamed-chunk-2" class="theorem"><strong>Theorem 3.1  </strong></span>For each <span class="math inline">\(\bar{y}\)</span> in <span class="math inline">\(W\)</span>, there exists a linear combination</p>
<p><span class="math display">\[
y = c_1\bar{u}_1 + \cdots + c_p\bar{u}_p
\]</span></p>
<p>and</p>
<p><span class="math display">\[
c_i = \frac{\bar{y} \cdot \bar{u}_i}{\bar{u}_i \cdot \bar{u}_i} \quad i = 1, \cdots, p
\]</span></p>
where <span class="math inline">\(\{\bar{u}_1, \dots, \bar{u}_p\}\)</span> is an orthogonal basis.
</div>

<div class="proof">
Proof
</div>
<p><span class="math display">\[
\begin{split}
\bar{u}_1 \cdot \bar{y} &amp;= \bar{u}_1 \cdot (c_1\bar{u}_1 + \cdots + c_p\bar{u}_p) \\
  &amp;= c_1 \bar{u}_1 \cdot \bar{u}_1
\end{split}
\]</span>
So:</p>
<p><span class="math display">\[
c_1 = \frac{\bar{u}_1 \cdot \bar{y}}{\bar{u}_1 \cdot \bar{u}_1}
\]</span></p>
<p>Derivations for other <span class="math inline">\(c_i\)</span> is similar.</p>
</div>
<div id="orthogonal-decomposition-1" class="section level3">
<h3><span class="header-section-number">3.1.3</span> Orthogonal Decomposition</h3>
<p><strong>Orthogonal decomposition</strong> split <span class="math inline">\(\bar{y}\)</span> in <span class="math inline">\(\mathbb{R^n}\)</span> into two vectors, one in <span class="math inline">\(W\)</span> and one in its orthogonal compliment <span class="math inline">\(W^{\perp}\)</span>.</p>

<div class="theorem">
<p><span id="thm:unnamed-chunk-3" class="theorem"><strong>Theorem 3.2  </strong></span>Let <span class="math inline">\(\mathbb{R}^n\)</span> be a inner product space and <span class="math inline">\(W\)</span> and subspace of <span class="math inline">\(\mathbb{R}^n\)</span>. Then every <span class="math inline">\(\bar{v}\)</span> in <span class="math inline">\(W\)</span> can be written uniquely in the form</p>
<p><span class="math display">\[
\bar{v} = \bar{v}_w + \bar{v}_{\perp}
\]</span></p>
where <span class="math inline">\(\bar{v}_w \in W\)</span> and <span class="math inline">\(\bar{v}_{\perp} \in W^{\perp}\)</span>
</div>

<div class="proof">
Proof
</div>
<p>Let <span class="math inline">\(\bar{u}_1, ..., \bar{u}_m\)</span> be a orthonormal basis for <span class="math inline">\(W\)</span>, there exists linear combination according to Section <a href="orthogonality.html#orthogonal-sets-and-orthogonal-basis">3.1.2</a></p>
<p><span class="math display">\[
\bar{v}_w = (\bar{v} \cdot \bar{u}_1)\bar{u}_1 + \cdots + (\bar{v} \cdot \bar{u}_m)\bar{u}_m
\]</span>
and</p>
<p><span class="math display">\[
\bar{v}_{\perp} = \bar{v} - \bar{v}_w
\]</span>
It is clear that <span class="math inline">\(\bar{v}_W \in W\)</span>. And we can also show that <span class="math inline">\(\bar{v}_{\perp}\)</span> is perpendicular to <span class="math inline">\(W\)</span></p>
<p><span class="math display">\[
\begin{split}
\bar{v}_{\perp} \cdot \bar{u}_i &amp;= [\bar{v}- (\bar{v} \cdot \bar{u}_1)\bar{u}_1 - \cdots - (\bar{v} \cdot \bar{u}_m)\bar{u}_m] \cdot \bar{u}_i \\
&amp;= (\bar{v} \cdot \bar{u}_1) - [(\bar{v} \cdot \bar{u}_i)\bar{u}_i \cdot \bar{u}_i] \\
&amp;= 0
\end{split}
\]</span></p>
<p>which implies <span class="math inline">\(\bar{v}_{\perp} \in W^{\perp}\)</span>.</p>
<p>To prove that <span class="math inline">\(\bar{v}_w\)</span> and <span class="math inline">\(\bar{v}_{\perp}\)</span> are unique (does not depend on the choice of basis), let <span class="math inline">\(\bar{u}_1&#39;, ..., \bar{u}_m&#39;\)</span> be another orthonormal basis for <span class="math inline">\(W\)</span>, and define <span class="math inline">\(\bar{v}_w&#39;\)</span> and <span class="math inline">\(\bar{v}_{\perp}&#39;\)</span> similarly we want to get <span class="math inline">\(\bar{v}_w&#39; = \bar{v}_w\)</span> and <span class="math inline">\(\bar{v}_{\perp}&#39; = \bar{v}_{\perp}\)</span>.</p>
<p>By definition</p>
<p><span class="math display">\[
\bar{v}_w + \bar{v}_{\perp} = \bar{v} = \bar{v}_w&#39; + \bar{v}_{\perp}&#39; 
\]</span></p>
<p>so</p>
<p><span class="math display">\[
\underbrace{\bar{v}_w - \bar{v}_w&#39;}_{\in W} = \underbrace{\bar{v}_{\perp}&#39; - \bar{v}_{\perp}}_{\in W^{\perp}}
\]</span>
From the orthogonality of these subspaces, we have</p>
<p><span class="math display">\[
0 = (\bar{v}_w - \bar{v}_w&#39;) \cdot (\bar{v}_{\perp}&#39; - \bar{v}_{\perp}) = (\bar{v}_w - \bar{v}_w&#39;) \cdot (\bar{v}_w - \bar{v}_w&#39;) = \|\bar{v}_w - \bar{v}_w&#39;\|^2
\]</span>
Similarly we have <span class="math inline">\(\|\bar{v}_{\perp}&#39; - \bar{v}_{\perp}\|^2 = 0\)</span>.</p>
<p>The existence and uniqueness of the decomposition above mean that</p>
<p><span class="math display">\[
\mathbb{R}^n = W \oplus W^{\perp}
\]</span></p>
<p>whenever <span class="math inline">\(W\)</span> is a subspace.</p>
</div>
</div>
<div id="orthonormal-sets-and-orthogonal-matrices" class="section level2">
<h2><span class="header-section-number">3.2</span> Orthonormal Sets and Orthogonal Matrices</h2>
<p>An orthogonal set whose components are all <em>unit vectors</em> is said to be <em>orthonormal</em> sets.</p>
<p>According this definition, we can easily create an orthonormal set using the original orthogonal set after perform scaling. If <span class="math inline">\(\{\bar{u}_1, \dots, \bar{u}_p\}\)</span> forms an orthogonal set in <span class="math inline">\(\mathbb{R^n}\)</span>, then an orthonormal set will be <span class="math inline">\(\{\bar{q}_1, \dots, \bar{q}_p\}\)</span></p>
<p><span class="math display">\[
\begin{aligned}
\bar{q}_1 &amp;= \frac{\bar{u}_1}{\|\bar{u}_1\|} \\
\bar{q}_2 &amp;= \frac{\bar{u}_2}{\|\bar{u}_2\|} \\
\vdots \\
\bar{q}_p &amp;= \frac{\bar{u}_p}{\|\bar{u}_p\|}
\end{aligned}
\]</span></p>
<div id="orthogonal-matrices" class="section level3">
<h3><span class="header-section-number">3.2.1</span> Orthogonal Matrices</h3>
<p>An <em>orthogonal matrix</em> is a square matrix <span class="math inline">\(Q\)</span> whose inverse is its transpose:</p>
<p><span class="math display" id="eq:orthogonal-matrix">\[
\tag{3.1}
QQ^T = Q^TQ = I
\]</span></p>
<p>Another way of defining it is that an orthogonal matrix has both <strong>orthonormal columns</strong> and <strong>orthonormal rows</strong>.</p>
<p>Orthogonal matrices have a nice property that they preserve inner products:</p>
<p><span class="math display">\[
(Q\bar{x})^T(Q\bar{y}) = \bar{x}^TQ^TQ\bar{y} = \bar{x}^TI\bar{y} = \bar{x}^T\bar{y}
\]</span></p>
<p>A direct result is that <span class="math inline">\(Q\)</span> preserves L2 norms</p>
<p><span class="math display">\[
\|Q\bar{x}\|_2 = \sqrt{(Q\bar{x})^T(Q\bar{x})} = \sqrt{\bar{x}^T\bar{x}} = \|\bar{x}\|_2
\]</span></p>
<p>Therefore multiplication by an orthogonal matrix can be considered as a transformation that preserves length, but may rotate or reflect the vector about the origin.</p>
<p>Note that <span class="math inline">\(Q\)</span> may not necessarily be a square matrix to satisfy <span class="math inline">\(Q^TQ = I\)</span>. For exmaple <span class="math inline">\(Q \in \mathbb{R}^{m \times n}, n &lt; m\)</span>, but its columns and rows can still be orthonormal, then <span class="math inline">\(QQ^T = I\)</span>. But in most cases the term orthogonal implies a square matrix <span class="math inline">\(Q\)</span>.</p>
</div>
<div id="best-approximation" class="section level3">
<h3><span class="header-section-number">3.2.2</span> Best Approximation</h3>

<div class="theorem">
<p><span id="thm:best-approximation" class="theorem"><strong>Theorem 3.3  (The Best Approximation)  </strong></span>Given <span class="math inline">\(\bar{y}\)</span> be any vector in <span class="math inline">\(\mathbb{R^n}\)</span>, with its subspace <span class="math inline">\(W\)</span>, let <span class="math inline">\(\hat{\bar{y}}\)</span> be the orthogonal projection of <span class="math inline">\(\bar{y}\)</span> onto <span class="math inline">\(W\)</span>. Then <span class="math inline">\(\hat{\bar{y}}\)</span> is the closest point in <span class="math inline">\(W\)</span> to <span class="math inline">\(\bar{y}\)</span> in the sense that</p>
<span class="math display">\[
\|\bar{y} - \hat{\bar{y}}\| \le \|\bar{y} - \bar{v}\|
\]</span>
</div>

<div class="proof">
Proof
</div>
<p>Take <span class="math inline">\(\bar{v}\)</span> distinct from <span class="math inline">\(\hat{\bar{y}}\)</span> in <span class="math inline">\(W\)</span>, we know that <span class="math inline">\(\bar{y} - \hat{\bar{y}}\)</span> is perpendicular to <span class="math inline">\(\bar{v}\)</span>. According to Pythoagorean theorem, we have</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-4"></span>
<img src="images/best-approximation.png" alt="figure from page p352, ch6 [@lay2006-3]" width="120%" />
<p class="caption">
Figure 2.2: figure from page p352, ch6 <span class="citation">(Lay <a href="references.html#ref-lay2006-3" role="doc-biblioref">2006</a>)</span>
</p>
</div>
<p><span class="math display">\[
\|\bar{y}-  \bar{v}\|^2 = \|\bar{\hat{y}} - \bar{v}\|^2 + \|\bar{y} -\bar{\hat{y}}\|^2 
\]</span>
When <span class="math inline">\(\bar{v}\)</span> is distinct from <span class="math inline">\(\bar{\hat{y}}\)</span>, <span class="math inline">\(\|\bar{\hat{y}} - \bar{v}\|^2\)</span> is non-negative, so the error term of choosing <span class="math inline">\(\bar{v}\)</span> is always larger than that of the orthogonal projection <span class="math inline">\(\bar{\hat{y}}\)</span>.</p>
</div>
</div>
<div id="projection-and-idempotent-matrices" class="section level2">
<h2><span class="header-section-number">3.3</span> Projection and idempotent matrices</h2>
<p><span class="math display">\[
\begin{split}
P_S\bar{v} &amp;= (\bar{v} \cdot \bar{u}_1)\bar{u}_1 + \cdots + (\bar{v} \cdot \bar{u}_m)\bar{u}_m \\
&amp;= \bar{v}^T\bar{u}_1\bar{u}_1 + \cdots +  \bar{v}^T\bar{u}_m\bar{u}_m\\
&amp;= (\bar{u}_1\bar{u}_1^T)\bar{v} + \cdots +  (\bar{u}_m\bar{u}_m^T)\bar{v} \\
&amp;= (\sum_{i=1}^{M}{\bar{u}_i\bar{u}_i^T})\bar{v}\\
&amp;= 
\begin{bmatrix}
\bar{u}_1 &amp; \cdots &amp; \bar{u}_m
\end{bmatrix}
\begin{bmatrix}
\bar{u}_1^T \\
\vdots \\
\bar{u}_m^T
\end{bmatrix}\bar{v} \\
&amp;= UU^T\bar{v}
\end{split}
\]</span></p>
<p>In practical problems, it is more convenient to use matrix at hand rather than producing an orthonormal basis.</p>

<div class="corollary">
<span id="cor:unnamed-chunk-5" class="corollary"><strong>Corollary 3.1  </strong></span>Idempotent matrices have eigenvalues either <span class="math inline">\(1\)</span> or <span class="math inline">\(0\)</span>.
</div>

<hr>
<p>Another way to derive projection matrices with matrix calculus</p>
<p><span class="math display">\[
\begin{split}
\|A\bar{x} - \bar{b}\|_2^2
&amp;= (A\bar{x} - \bar{b})^T(A\bar{x} - \bar{b}) \\
&amp;= \bar{x}^TA^TA\bar{x} - 2\bar{b}^TA\bar{x} + \bar{b}^T\bar{b}
\end{split}
\]</span></p>
<p><span class="math display">\[
\begin{split}
\nabla_{x}(\bar{x}^TA^TA\bar{x} - 2A\bar{x}\bar{b} + \bar{b}^T\bar{b}) &amp;=\nabla_x(\bar{x}^TA^TA\bar{x}) - \nabla_x{2\bar{b}^TA\bar{x}} + \nabla_x{\bar{b}^T\bar{b}} \\
&amp;= 2(A^TA)\bar{x} - 2A^T\bar{b}
\end{split}
\]</span></p>
<p><span class="math display">\[
\bar{x} = (A^TA)^{-1}A^T\bar{b}
\]</span>
<span class="math display">\[
A\bar{x} = A(A^TA)^{-1}A^T\bar{b} = \hat{\bar{b}}
\]</span></p>
</div>
<div id="gram-schmidt-process" class="section level2">
<h2><span class="header-section-number">3.4</span> Gram-Schmidt Process</h2>
<p>Then Gram-Schmidt process is a simple algorithm that transforms a set of linearly independent vectors into orthogonal or orthonormal basis for a subspace. In its essence, it is a sequential projection of <span class="math inline">\(\bar{x}_{i}\)</span> onto the space spanned by the previously created orthogonal set <span class="math inline">\(\{\bar{v}_{1}, ..., \bar{v}_{i-1}\}\)</span>, and take the term in the orthogonal compliment to be <span class="math inline">\(\bar{v}_{i}\)</span>.</p>

<div class="theorem">
<p><span id="thm:gram-schmidt" class="theorem"><strong>Theorem 3.4  (the Gram-Schmidt process)  </strong></span>Given a basis <span class="math inline">\(\{\bar{x}_1, ..., \bar{x}_p\}\)</span> for a nonzero subspace <span class="math inline">\(W\)</span> of <span class="math inline">\(\mathbb{R}^n\)</span>, define</p>
<p><span class="math display">\[
\begin{aligned}
\bar{v}_1 &amp;= \bar{x}_1 \\
\bar{v}_2 &amp;= \bar{x}_2 - \frac{\bar{x}_2 \cdot \bar{v}_1}{\bar{v}_1 \cdot \bar{v}_1}\bar{v}_1 \\
\bar{v}_3 &amp;= \bar{x}_3 
- \frac{\bar{x}_3 \cdot \bar{v}_1}{\bar{v}_1 \cdot \bar{v}_1}\bar{v}_1 
- \frac{\bar{x}_3 \cdot \bar{v}_2}{\bar{v}_2 \cdot \bar{v}_2}\bar{v}_2
\\
&amp; \vdots \\
\bar{v}_p &amp;= \bar{x}_p 
- \frac{\bar{x}_p \cdot \bar{v}_1}{\bar{v}_1 \cdot \bar{v}_1}\bar{v}_1 
- \frac{\bar{x}_p \cdot \bar{v}_2}{\bar{v}_2 \cdot \bar{v}_2}\bar{v}_2
- \cdots
- \frac{\bar{x}_p \cdot \bar{v}_{p-1}}{\bar{v}_{p-1} \cdot \bar{v}_{p-1}}\bar{v}_{p-1}
\end{aligned}
\]</span></p>
<p>Then <span class="math inline">\(\{\bar{v}_1, ..., \bar{v}_p\}\)</span> is an orthogonal basis for <span class="math inline">\(W\)</span>. In addition</p>
<span class="math display">\[
\text{Span}\{\bar{v}_1, ..., \bar{v}_p\} = \text{Span}\{\bar{x}_1, ..., \bar{x}_p\} 
\]</span>
</div>

<p>To make <span class="math inline">\(\{\bar{v}_1, ..., \bar{v}_p\}\)</span> an <em>orthonormal</em> basis, there is simply one more step of normalization</p>
<p><span class="math display">\[
\{\bar{q}_i = \frac{\bar{v}_i}{\|\bar{v}_i\|}, \;i = 1, ... p\} 
\]</span></p>
<p>The Gram-Schmidt basis does not expose any specific properties of a vector with the help of its coordinates. The discrete cosine transform
uses a basis with trigonometric properties in order to expose periodicity in a time series. (see Section 2.7.3 of LAOML <span class="citation">(Aggarwal <a href="references.html#ref-DBLP:books/sp/Aggarwal20" role="doc-biblioref">2020</a>)</span>)</p>
</div>
<div id="qr-factorizaiton" class="section level2">
<h2><span class="header-section-number">3.5</span> QR Factorizaiton</h2>
<p>For <span class="math inline">\(A \in \mathbb{R}^{m \times n}\)</span> with linearly independent columns <span class="math inline">\(\bar{x}_1, ..., \bar{x}_n\)</span>, apply the Gram-Schmidt process to <span class="math inline">\(\bar{x}_1, ..., \bar{x}_n\)</span> amounts to <em>factorizing</em> <span class="math inline">\(A\)</span>.</p>

<div class="theorem">
<span id="thm:qr" class="theorem"><strong>Theorem 3.5  (QR factorization)  </strong></span>if <span class="math inline">\(A\)</span> is an <span class="math inline">\(m \times n\)</span> matrix with full column rank, then <span class="math inline">\(A\)</span> can be factored as <span class="math inline">\(A = QR\)</span>, where <span class="math inline">\(Q\)</span> is an <span class="math inline">\(m \times n\)</span> matrix whose columns form an orthonormal basis of <span class="math inline">\(\text{Col}\;A\)</span> and <span class="math inline">\(R\)</span> is an <span class="math inline">\(n \times n\)</span> upper triangular invertible matrix with positive entries on its diagonal.
</div>

<div class="proof">
Proof
</div>
<p>Because <span class="math inline">\(A_{m \times n}\)</span> is full column rank, we can transform its column vector <span class="math inline">\(\{\bar{a}_{1}, ..., \bar{a}_{n}\}\)</span> into a new set of orthonormal basis <span class="math inline">\(\{\bar{q}_{1}, ..., \bar{q}_{n}\}\)</span> with Gram-Schmidt process. Let</p>
<p><span class="math display">\[
Q = [\bar{q}_{1} \;\; \cdots \;\; \bar{q}_{n}]
\]</span></p>
<p>To find <span class="math inline">\(R\)</span>, let’s consider how the Gram-Schmidt process works. We create <span class="math inline">\(\{\bar{q}_{1}, \cdots, \bar{q}_{n}\}\)</span> by (for simplicity exclude the vector symbol)</p>
<p><span class="math display">\[
\begin{aligned}
u_1 &amp;= a_1 &amp;&amp; q_1 = \frac{u_1}{\|u_1\|} \\
u_2 &amp;= a_2 - (a_2 \cdot u_1)u_1 &amp;&amp; q_2 = \frac{u_2}{\|u_2 \|} \\
u_3 &amp;=  a_3 - (a_3 \cdot u_1)u_1 - (a_3 \cdot u_2)u_2 &amp;&amp; q_3 = \frac{q_3}{\|q_3\|} \\ 
\vdots  \\
u_n  &amp;= a_n - \sum_{i = 1}^{n}(a_n \cdot u_i)u_i &amp;&amp; q_n = \frac{u_n}{\|u_n\|}
\end{aligned}
\]</span></p>
<p>Then solve for <span class="math inline">\(a_i\)</span> over the newly produced orthonormal basis. Recall the geometric interpretation of Gram-Schmidt Process, the representation of <span class="math inline">\(a_1\)</span> depends only on <span class="math inline">\(q_1\)</span>, <span class="math inline">\(a_2\)</span> depends only on <span class="math inline">\(q_1\)</span> and <span class="math inline">\(q_2\)</span>, <span class="math inline">\(a_3\)</span> depends only on <span class="math inline">\(q_1, q_2, q_3\)</span> and so on</p>
<p><span class="math display">\[
\begin{aligned}
a_1 &amp;=  (q_1 \cdot a_1) q_1 \\
a_2 &amp;= (q_1 \cdot a_2) q_1 + (q_2 \cdot a_2)q_2 \\
a_3 &amp;= (q_1 \cdot a_3)q_1 + (q_2 \cdot a_3)q_2 + (q_3 \cdot a_3)q_3  \\
\vdots \\
a_n &amp;= \sum_{i=1}^{n}(q_i \cdot a_n)q_i
\end{aligned}
\]</span>
The set equations above is essentially expressing <span class="math inline">\(a_i\)</span> by their coordinates associated with the orthonormal basis <span class="math inline">\(q_i\)</span>. As a summary, for any column of <span class="math inline">\(A\)</span>, there exists a set of constant <span class="math inline">\(r_{1k}, ..., r_{kk}\)</span> where <span class="math inline">\(r_{ik} = q_i \cdot a_k\)</span>, such that</p>
<p><span class="math display">\[
\bar{x}_k = r_{1k}\bar{q}_{1} + \cdots + r_{kk}\bar{q}_{k} + 0 \cdot\bar{q}_{k+1} + \cdots + 0 \cdot \bar{q}_{n}
\]</span></p>
<p>Therefore</p>
<p><span class="math display">\[
A = [\bar{x}_{1} \;\; \bar{x}_{2} \;\; \cdots \;\; \bar{x}_{n}] = [\bar{q}_{1} \;\; \bar{q}_{2} \;\; \cdots \;\; \bar{q}_{n}] 
\begin{bmatrix}
r_{11} &amp; r_{12} &amp; \cdots &amp; r_{1n} \\
0 &amp; r_{22} &amp; \cdots &amp; r_{2n} \\
\vdots &amp; \vdots &amp; &amp; \vdots\\
0 &amp; 0 &amp; \cdots &amp; r_{nn}
\end{bmatrix} 
= QR
\]</span>
We could assume that <span class="math inline">\(r_{kk} \ge 0\)</span>. (if <span class="math inline">\(r_{kk} &lt; 0\)</span>, multiply both <span class="math inline">\(r_{kk}\)</span> and <span class="math inline">\(\bar{u}_k\)</span> by <span class="math inline">\(-1\)</span>)</p>
</div>
<div id="complexity" class="section level2">
<h2><span class="header-section-number">3.6</span> Complexity</h2>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="vector-spaces.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="eigenthings-and-quadratic-forms.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/enixam/math-foundations/edit/master/orthogonality.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
