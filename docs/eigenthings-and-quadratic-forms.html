<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 4 Eigenthings and quadratic forms | Mathematical Notes for Machine Learning</title>
  <meta name="description" content="Chapter 4 Eigenthings and quadratic forms | Mathematical Notes for Machine Learning" />
  <meta name="generator" content="bookdown 0.20 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 4 Eigenthings and quadratic forms | Mathematical Notes for Machine Learning" />
  <meta property="og:type" content="book" />
  
  
  
  <meta name="github-repo" content="enixam/math-foundations" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 4 Eigenthings and quadratic forms | Mathematical Notes for Machine Learning" />
  
  
  

<meta name="author" content="Qiushi Yan" />


<meta name="date" content="2020-07-20" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="orthogonality.html"/>
<link rel="next" href="matrix-calculus.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>



<link rel="stylesheet" href="css\style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Math foundations in Machine learning</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="part"><span><b>I Linear Algebra</b></span></li>
<li class="chapter" data-level="1" data-path="matrix-basics.html"><a href="matrix-basics.html"><i class="fa fa-check"></i><b>1</b> Matrix basics</a><ul>
<li class="chapter" data-level="1.1" data-path="matrix-basics.html"><a href="matrix-basics.html#matrix-multiplication"><i class="fa fa-check"></i><b>1.1</b> Matrix multiplication</a></li>
<li class="chapter" data-level="1.2" data-path="matrix-basics.html"><a href="matrix-basics.html#elemetary-matrix-and-row-operations"><i class="fa fa-check"></i><b>1.2</b> Elemetary matrix and row operations</a></li>
<li class="chapter" data-level="1.3" data-path="matrix-basics.html"><a href="matrix-basics.html#lu-factorization"><i class="fa fa-check"></i><b>1.3</b> LU factorization</a></li>
<li class="chapter" data-level="1.4" data-path="matrix-basics.html"><a href="matrix-basics.html#determinants"><i class="fa fa-check"></i><b>1.4</b> Determinants</a><ul>
<li class="chapter" data-level="1.4.1" data-path="matrix-basics.html"><a href="matrix-basics.html#cofactor-expansion"><i class="fa fa-check"></i><b>1.4.1</b> Cofactor expansion</a></li>
<li class="chapter" data-level="1.4.2" data-path="matrix-basics.html"><a href="matrix-basics.html#geometric-interpretation-of-determinant"><i class="fa fa-check"></i><b>1.4.2</b> Geometric interpretation of determinant</a></li>
<li class="chapter" data-level="1.4.3" data-path="matrix-basics.html"><a href="matrix-basics.html#properties-of-determinant"><i class="fa fa-check"></i><b>1.4.3</b> Properties of determinant</a></li>
<li class="chapter" data-level="1.4.4" data-path="matrix-basics.html"><a href="matrix-basics.html#cramers-rule"><i class="fa fa-check"></i><b>1.4.4</b> Cramer’s rule</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="matrix-basics.html"><a href="matrix-basics.html#trace"><i class="fa fa-check"></i><b>1.5</b> Trace</a></li>
<li class="chapter" data-level="1.6" data-path="matrix-basics.html"><a href="matrix-basics.html#inverse-of-a-matrix"><i class="fa fa-check"></i><b>1.6</b> Inverse of a matrix</a></li>
<li class="chapter" data-level="1.7" data-path="matrix-basics.html"><a href="matrix-basics.html#matrix-multiplication-as-linear-transformation"><i class="fa fa-check"></i><b>1.7</b> Matrix multiplication as linear transformation</a></li>
<li class="chapter" data-level="1.8" data-path="matrix-basics.html"><a href="matrix-basics.html#statistics-and-proabability"><i class="fa fa-check"></i><b>1.8</b> Statistics and proabability</a><ul>
<li class="chapter" data-level="1.8.1" data-path="matrix-basics.html"><a href="matrix-basics.html#sample-statistics"><i class="fa fa-check"></i><b>1.8.1</b> Sample statistics</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="vector-spaces.html"><a href="vector-spaces.html"><i class="fa fa-check"></i><b>2</b> Vector spaces</a><ul>
<li class="chapter" data-level="2.1" data-path="vector-spaces.html"><a href="vector-spaces.html#four-subspaces"><i class="fa fa-check"></i><b>2.1</b> Four subspaces</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="orthogonality.html"><a href="orthogonality.html"><i class="fa fa-check"></i><b>3</b> Orthogonality</a><ul>
<li class="chapter" data-level="3.1" data-path="orthogonality.html"><a href="orthogonality.html#metric-spaces-normed-spaces-inner-product-spaces"><i class="fa fa-check"></i><b>3.1</b> Metric spaces, normed spaces, inner product spaces</a></li>
<li class="chapter" data-level="3.2" data-path="orthogonality.html"><a href="orthogonality.html#orthogonal-decomposition"><i class="fa fa-check"></i><b>3.2</b> Orthogonal decomposition</a><ul>
<li class="chapter" data-level="3.2.1" data-path="orthogonality.html"><a href="orthogonality.html#orthogonal-complements"><i class="fa fa-check"></i><b>3.2.1</b> Orthogonal complements</a></li>
<li class="chapter" data-level="3.2.2" data-path="orthogonality.html"><a href="orthogonality.html#orthogonal-sets-and-orthogonal-basis"><i class="fa fa-check"></i><b>3.2.2</b> Orthogonal sets and orthogonal basis</a></li>
<li class="chapter" data-level="3.2.3" data-path="orthogonality.html"><a href="orthogonality.html#orthogonal-decomposition-1"><i class="fa fa-check"></i><b>3.2.3</b> Orthogonal decomposition</a></li>
<li class="chapter" data-level="3.2.4" data-path="orthogonality.html"><a href="orthogonality.html#best-approximation"><i class="fa fa-check"></i><b>3.2.4</b> Best approximation</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="orthogonality.html"><a href="orthogonality.html#gram-schmidt-process"><i class="fa fa-check"></i><b>3.3</b> Gram-Schmidt process</a><ul>
<li class="chapter" data-level="3.3.1" data-path="orthogonality.html"><a href="orthogonality.html#qr-factorizaiton"><i class="fa fa-check"></i><b>3.3.1</b> QR factorizaiton</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="orthogonality.html"><a href="orthogonality.html#orthonormal-sets-and-orthogonal-matrices"><i class="fa fa-check"></i><b>3.4</b> Orthonormal sets and orthogonal matrices</a><ul>
<li class="chapter" data-level="3.4.1" data-path="orthogonality.html"><a href="orthogonality.html#orthogonal-matrices"><i class="fa fa-check"></i><b>3.4.1</b> Orthogonal matrices</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="eigenthings-and-quadratic-forms.html"><a href="eigenthings-and-quadratic-forms.html"><i class="fa fa-check"></i><b>4</b> Eigenthings and quadratic forms</a><ul>
<li class="chapter" data-level="4.1" data-path="eigenthings-and-quadratic-forms.html"><a href="eigenthings-and-quadratic-forms.html#eigenvectors-and-eigenvalues"><i class="fa fa-check"></i><b>4.1</b> Eigenvectors and eigenvalues</a></li>
<li class="chapter" data-level="4.2" data-path="eigenthings-and-quadratic-forms.html"><a href="eigenthings-and-quadratic-forms.html#diagnolization-and-similar-matrices"><i class="fa fa-check"></i><b>4.2</b> Diagnolization and similar matrices</a><ul>
<li class="chapter" data-level="4.2.1" data-path="eigenthings-and-quadratic-forms.html"><a href="eigenthings-and-quadratic-forms.html#similarity"><i class="fa fa-check"></i><b>4.2.1</b> similarity</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="eigenthings-and-quadratic-forms.html"><a href="eigenthings-and-quadratic-forms.html#symmetric-matrices"><i class="fa fa-check"></i><b>4.3</b> Symmetric matrices</a><ul>
<li class="chapter" data-level="4.3.1" data-path="eigenthings-and-quadratic-forms.html"><a href="eigenthings-and-quadratic-forms.html#spectral-decomposition"><i class="fa fa-check"></i><b>4.3.1</b> Spectral decomposition</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="eigenthings-and-quadratic-forms.html"><a href="eigenthings-and-quadratic-forms.html#svd"><i class="fa fa-check"></i><b>4.4</b> SVD</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="matrix-calculus.html"><a href="matrix-calculus.html"><i class="fa fa-check"></i><b>5</b> Matrix calculus</a></li>
<li class="part"><span><b>II Calculus</b></span></li>
<li class="chapter" data-level="6" data-path="taylor-series-and-expansion.html"><a href="taylor-series-and-expansion.html"><i class="fa fa-check"></i><b>6</b> Taylor series and expansion</a></li>
<li class="part"><span><b>III Applications</b></span></li>
<li class="chapter" data-level="7" data-path="linear-models.html"><a href="linear-models.html"><i class="fa fa-check"></i><b>7</b> Linear models</a><ul>
<li class="chapter" data-level="7.1" data-path="linear-models.html"><a href="linear-models.html#least-square-estimation"><i class="fa fa-check"></i><b>7.1</b> Least square estimation</a></li>
<li class="chapter" data-level="7.2" data-path="linear-models.html"><a href="linear-models.html#maximum-likelihood-estimation"><i class="fa fa-check"></i><b>7.2</b> Maximum likelihood estimation</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="principle-component-analysis.html"><a href="principle-component-analysis.html"><i class="fa fa-check"></i><b>8</b> Principle component analysis</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li>
  <a href="https://github.com/rstudio/bookdown" target="blank">Written with bookdown</a>
</li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Mathematical Notes for Machine Learning</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="eigenthings-and-quadratic-forms" class="section level1">
<h1><span class="header-section-number">Chapter 4</span> Eigenthings and quadratic forms</h1>
<div id="eigenvectors-and-eigenvalues" class="section level2">
<h2><span class="header-section-number">4.1</span> Eigenvectors and eigenvalues</h2>

<div class="definition">
<p><span id="def:eigen" class="definition"><strong>Definition 4.1  (Eigenvectors and eigenvalues)  </strong></span>An <strong>eigenvector</strong> of an <span class="math inline">\(n \times n\)</span> matrix <span class="math inline">\(A\)</span> is a <em>nonzero</em> vector <span class="math inline">\(\boldsymbol{x}\)</span> such that <span class="math inline">\(A\boldsymbol{x} = \lambda\boldsymbol{x}\)</span>.</p>
<span class="math inline">\(\lambda\)</span> is the <strong>eigenvalue</strong> of <span class="math inline">\(A\)</span> if there is a nontrivial solution <span class="math inline">\(\boldsymbol{x}\)</span> of <span class="math inline">\(A\boldsymbol{x} = \lambda \boldsymbol{x}\)</span>; such an <span class="math inline">\(\boldsymbol{x}\)</span> is called an <em>eigenvector corresponding to <span class="math inline">\(\lambda\)</span></em>
</div>

<p>To find eigenvalues and corresponding eigenvectors of <span class="math inline">\(A\)</span>, we look at the equation</p>
<p><span class="math display">\[
(A - \lambda I)\boldsymbol{x}= 0
\]</span></p>
<p>Since eigenvector <span class="math inline">\(\boldsymbol{x}\)</span> must be nonzero, <span class="math inline">\((A - \lambda I)\)</span> is a singular matrix</p>
<p><span class="math display" id="eq:characteristic-equation">\[\begin{equation}
\tag{4.1}
\det (A - \lambda I) = 0
\end{equation}\]</span></p>
<p>Eq <a href="eigenthings-and-quadratic-forms.html#eq:characteristic-equation">(4.1)</a> is called the <strong>characteristic equation</strong> of matrix <span class="math inline">\(A\)</span>. This is a scalar equation containing information about eigenvalues and eigenvectors of a square matrix <span class="math inline">\(A\)</span>.</p>

<div class="theorem">
<span id="thm:unnamed-chunk-1" class="theorem"><strong>Theorem 1.1  </strong></span>Eigenvalues of a trangular matrix are its diagonal entries.
</div>

<p><strong>PROOF</strong></p>
<p>Consider the <span class="math inline">\(3 \times 3\)</span> case. If <span class="math inline">\(A\)</span> is upper triangular, then <span class="math inline">\(A - \lambda I\)</span> has the form</p>
<p><span class="math display">\[
\begin{bmatrix}
a_{11} - \lambda &amp; a_{12} &amp; a_{13} \\
0 &amp; a_{22} - \lambda &amp; a_{23}  \\
0 &amp; 0 &amp; a_{33} - \lambda
\end{bmatrix}
\]</span>
So the roots of characteristic are <span class="math inline">\(a_{11}, a_{22}, a_{33}\)</span> respectively.</p>
<p>There are some useful results about how eigenvalues change after various manipulations.</p>
<ol style="list-style-type: decimal">
<li>For any <span class="math inline">\(k, b \in \mathbb{R}\)</span>, <span class="math inline">\(\boldsymbol{x}\)</span> is an eigenvector of <span class="math inline">\(kA + bI\)</span> with eigenvalue <span class="math inline">\(k\lambda + b\)</span></li>
</ol>
<p><strong>PROOF</strong>
<span class="math display">\[
(kA + bI)\boldsymbol{x} = kA\boldsymbol{x} + bI\boldsymbol{x} = k \lambda\boldsymbol{x} + b\boldsymbol{x} = (k\lambda + b)\boldsymbol{x} 
\]</span></p>
<p>2, If <span class="math inline">\(A\)</span> is invertible, then <span class="math inline">\(\boldsymbol{x}\)</span> is an eigenvector of <span class="math inline">\(A^{-1}\)</span> with eigenvalue <span class="math inline">\(1/\lambda\)</span></p>
<p><strong>PROOF</strong></p>
<p><span class="math display">\[
\boldsymbol{x} = A^{-1}A\boldsymbol{x} =  A^{-1}\lambda \boldsymbol{x} = \lambda A^{-1}\boldsymbol{x}
\]</span>
3. <span class="math inline">\(A^{k}\boldsymbol{x} = \lambda^{k}\boldsymbol{x}\)</span></p>
<p>The next theorem is important in terms of diagonalization and spectral decomposition</p>

<div class="theorem">
<span id="thm:distinct-eigenvalue" class="theorem"><strong>Theorem 4.1  </strong></span>For distinct eigenvalues <span class="math inline">\(\lambda_1, \cdots, \lambda_r\)</span> of an <span class="math inline">\(n \times n\)</span> matrix A, their corresponding eigenvectors <span class="math inline">\(\boldsymbol{v_1}, ..., \boldsymbol{v_r}\)</span> are linearly independent.
</div>

<p><strong>PROOF</strong></p>
<p>Suppose for r distinct eigenvalue <span class="math inline">\(\lambda_1, \cdots, \lambda_r\)</span>, the set <span class="math inline">\(\{\boldsymbol{v_1}, ..., \boldsymbol{v_r}\}\)</span> is not linearly independent, and <span class="math inline">\(p\)</span> is the least index such that <span class="math inline">\(\boldsymbol{v}_{p+1}\)</span> is a linear combination of the preceding vectors. Then there exists scalars <span class="math inline">\(c_1, \cdots, c_p\)</span> such that</p>
<p><span class="math display">\[
c_1\boldsymbol{v}_1 + \cdots + c_p\boldsymbol{v}_p = \boldsymbol{v}_{p+1} \tag{1}
\]</span>
Left multiply by <span class="math inline">\(A\)</span>, and note we have <span class="math inline">\(A\boldsymbol{v}_i = \lambda_i\boldsymbol{v}_i\)</span> for <span class="math inline">\(i = 1, ..., n\)</span></p>
<p><span class="math display">\[
c_1\lambda_1\boldsymbol{v}_1 + \cdots + c_p\lambda_p\boldsymbol{v}_p = \lambda_{p+1}\boldsymbol{v}_{p+1} \tag{2}
\]</span>
Multiplying both sides of (2) by <span class="math inline">\(\lambda_{p+1}\)</span> and subtracting (2) from the result</p>
<p><span class="math display">\[
c_1(\lambda_1 - \lambda_{p+1})\boldsymbol{v}_1 +\cdots + c_p(\lambda_p - \lambda_{p+1})\boldsymbol{v}_p = 0 \tag{3}
\]</span>
Since <span class="math inline">\(\boldsymbol{v}_1, ..., \boldsymbol{v}_p\)</span> are linearly independent, weights in (3) must be all zero. Since <span class="math inline">\(\lambda_1, \cdots, \lambda_p\)</span> are distinct, hence <span class="math inline">\(c_i = 0, \, i = 1, ..., p\)</span>. But then (5) says that eigenvector <span class="math inline">\(\boldsymbol{v}_{p+1}\)</span> is zero vector, which contradicts definition <a href="eigenthings-and-quadratic-forms.html#def:eigen">4.1</a></p>
</div>
<div id="diagnolization-and-similar-matrices" class="section level2">
<h2><span class="header-section-number">4.2</span> Diagnolization and similar matrices</h2>

<div class="definition">
<p><span id="def:diagonalization" class="definition"><strong>Definition 4.2  (Diagonalization thoerem)  </strong></span>An <span class="math inline">\(n \ times n\)</span> matrix <span class="math inline">\(A\)</span> is diagnolizable <strong>if and only if</strong> A has <span class="math inline">\(n\)</span> independent linearly independent eigenvectors.</p>
<p>In such case, in <span class="math inline">\(A = PDP^{-1}\)</span>, the diagonal entries of <span class="math inline">\(D\)</span> are eigenvalues that correpond, respectively, to the eigenvectors of in <span class="math inline">\(P\)</span></p>
In other words, <span class="math inline">\(A\)</span> is diagnolizable if and only if there are enough eigenvectors in form a basis of <span class="math inline">\(R^n\)</span>, called an <strong>eigenvector basis</strong> of <span class="math inline">\(R^n\)</span>
</div>

<p><strong>Proof</strong></p>
<p><span class="math display">\[
\begin{split}
AP &amp;= A[\boldsymbol{v}_1 \cdots \boldsymbol{v}_n] \\
   &amp;= [A\boldsymbol{v}_1 \cdots A\boldsymbol{v}_n] \\ 
   &amp;= [\lambda_1\boldsymbol{v}_1 \cdots \lambda_n\boldsymbol{v}_n]
\end{split}
\]</span>
while on the other side of the equation:</p>
<p><span class="math display">\[
\begin{aligned}
DP &amp;= 
[\boldsymbol{v}_1 \cdots \boldsymbol{v}_n]
\begin{bmatrix}
\lambda_1 &amp; 0 &amp; \cdots &amp; 0\\
0  &amp; \lambda_2 &amp; \cdots &amp; 0 \\
\vdots &amp; \vdots &amp; &amp; \vdots \\
0 &amp; 0 &amp; \cdots &amp; \lambda_n 
\end{bmatrix} 
 \\
&amp;= [\lambda_1\boldsymbol{v}_1 \cdots \lambda_n\boldsymbol{v}_n]
\end{aligned}
\]</span></p>
<p>So that</p>
<p><span class="math display">\[
\begin{aligned}
AP &amp;= PD \\
A &amp;= PDP^{-1}
\end{aligned}
\]</span>
Because <span class="math inline">\(P\)</span> contains <span class="math inline">\(n\)</span> independent columns so it’s invertible.</p>
<p>According to theorem <a href="eigenthings-and-quadratic-forms.html#thm:distinct-eigenvalue">4.1</a>, an <span class="math inline">\(n \times n\)</span> matrix with <span class="math inline">\(n\)</span> distinct eigenvalues is diagonalizable. This is a sufficient condition.</p>
<p>For matrices whose eigenvalues are not distinct, there is still a change that it is diagonalizable. For any matrix <span class="math inline">\(A_{n\times n}\)</span>, as long as the sum of the dimensions of the eigenspaces equals <span class="math inline">\(n\)</span> then <span class="math inline">\(P\)</span> is invertible. This could happen in the following two scenarios</p>
<ol style="list-style-type: decimal">
<li><p>The characteristic polynomial factors completely into linear factors. This is the case when <span class="math inline">\(A\)</span> has n distinct eigenvalues.</p></li>
<li><p>The dimension of the eigenspace for each <span class="math inline">\(\lambda_k\)</span> equals the multiplicity of <span class="math inline">\(\lambda_k\)</span>. Thus <span class="math inline">\(A\)</span> with repeated eigenvalues can still be diagonalizable.</p></li>
</ol>
<div id="similarity" class="section level3">
<h3><span class="header-section-number">4.2.1</span> similarity</h3>
<p>If <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are <span class="math inline">\(n \times n\)</span> matrices, then <span class="math inline">\(A\)</span> <strong>is similar to</strong> <span class="math inline">\(N\)</span> if there is an invertible matrix <span class="math inline">\(P\)</span> such that <span class="math inline">\(P^{-1}AP = B\)</span>, or equivalently if we write <span class="math inline">\(Q\)</span> for <span class="math inline">\(P^{-1}\)</span>, <span class="math inline">\(Q^{-1}BQ = A\)</span>. Changing <span class="math inline">\(A\)</span> into <span class="math inline">\(P^{-1}AP\)</span> is called a <strong>similarity transformation</strong>.</p>

<div class="theorem">
<span id="thm:unnamed-chunk-2" class="theorem"><strong>Theorem 3.1  </strong></span>If <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are similar, they have the same eigenvalues.
</div>

<p><strong>PROOF</strong><br />
If <span class="math inline">\(B = P^{-1}AP\)</span>, then</p>
<p><span class="math display">\[
B - \lambda I = P^{-1}AP - \lambda P^{-1}P = P^{-1}(AP - \lambda P) =  P^{-1}(A - \lambda I) P
\]</span>
so that</p>
<p><span class="math display">\[
\det (B - \lambda I ) = \det(P) \cdot \det(A - \lambda I ) \cdot \det(P^{-1})
\]</span></p>
</div>
</div>
<div id="symmetric-matrices" class="section level2">
<h2><span class="header-section-number">4.3</span> Symmetric matrices</h2>
<p>A <em>square</em> matrix <span class="math inline">\(A \in \mathbb{R}^{n \times n}\)</span> is <em>symmetric</em> if <span class="math inline">\(A = A^{T}\)</span>, and <em>anti-symmetric</em> if <span class="math inline">\(A = - A^{T}\)</span>.</p>
<p>It can be shown that for any <span class="math inline">\(A \in \mathbb{R}^{n \times n}\)</span>, <span class="math inline">\(A + A^T\)</span> is symmetric and <span class="math inline">\(A - A^T\)</span> anti-symmetric. So any square matrix <span class="math inline">\(A\)</span> can be wrote as a sum of a symmetric matrix and an anti-symmetric matrix</p>
<p><span class="math display">\[
A = \frac{1}{2}(A + A^T) + \frac{1}{2}(A - A^T)
\]</span></p>
<p>It is common to denote the set of all symmetric matrices of size <span class="math inline">\(n\)</span> as <span class="math inline">\(\mathbb{S}^n\)</span>, and <span class="math inline">\(A \in \mathbb{S}^n\)</span> means <span class="math inline">\(A\)</span> is a symmetric <span class="math inline">\(n \times n\)</span> matrix.</p>
<p>Symmetric matrices have some nice properties about diagonalization.</p>

<div class="theorem">
<span id="thm:unnamed-chunk-3" class="theorem"><strong>Theorem 3.2  </strong></span>If <span class="math inline">\(A\)</span> is symmetric, eigenvectors from distinct eigenvalues are <strong>orthogonal</strong>.
</div>

<p><strong>PROOF</strong></p>
<p>Let <span class="math inline">\(\boldsymbol{v}_1\)</span> and <span class="math inline">\(\boldsymbol{v}_2\)</span> be eigenvectors that correspond to distinct eigenvalues <span class="math inline">\(\lambda_1\)</span> and <span class="math inline">\(\lambda_2\)</span>. Compute</p>
<p><span class="math display">\[
\begin{split}
\lambda_1\boldsymbol{v}_1 \cdot \boldsymbol{v}_2 &amp;= (\lambda_1\boldsymbol{v}_1)^T\boldsymbol{v}_2 \\
&amp;= (\boldsymbol{v}_1^TA^T)\boldsymbol{v}_2 \\
&amp;= \boldsymbol{v}_1^T(A\boldsymbol{v}_2) \\
&amp;= \boldsymbol{v}_1^T(\lambda_2\boldsymbol{v}_2) \\
&amp;= \lambda_2\boldsymbol{v}_1 \cdot \boldsymbol{v}_2
\end{split}
\]</span>
because <span class="math inline">\(\lambda_1 \not = \lambda_2\)</span>, <span class="math inline">\(\boldsymbol{v}_1 \cdot \boldsymbol{v}_2 = 0\)</span>.</p>
<p>For symmetric matrices <span class="math inline">\(A \in \mathbb{R}^{n \times n}\)</span> without <span class="math inline">\(n\)</span> distinct eigenvalues, it turns out that the dimension of the eigenspace for each <span class="math inline">\(\lambda_k\)</span> always equals the multiplicity of <span class="math inline">\(\lambda_k\)</span>. For this reason, if <span class="math inline">\(A\)</span> is a symmetric matrix we can always construct a orthonormal set <span class="math inline">\(\{\boldsymbol{q}_1 \;\; \cdots \;\; \boldsymbol{q}_n\}\)</span> from <span class="math inline">\(\{\boldsymbol{v}_1 \;\; \cdots \;\; \boldsymbol{v}_n\}\)</span> such that</p>
<p><span class="math display">\[
P^{T} = 
\begin{bmatrix}
\boldsymbol{q}_1^T \\
\vdots \\ 
\boldsymbol{q}_n^T
\end{bmatrix}
= P^{-1}
\]</span>
Recall that matrix <span class="math inline">\(A\)</span> with <span class="math inline">\(n\)</span> linearly dependent eigenvectors is diagonalizable and can be written as</p>
<p><span class="math display">\[
A = PDP^{-1}
\]</span>
where <span class="math inline">\(P = [\boldsymbol{v}_1 \;\; \cdots \;\; \boldsymbol{v}_n]\)</span> and <span class="math inline">\(D\)</span> is a diagonal matrix with eigenvalues on its diagonal entries.</p>
<p>With symmetric matrices, after proper transformation we have <span class="math inline">\(P^T = P^{-1}\)</span>, so that</p>
<p><span class="math display" id="eq:orthogonal-diagonalization">\[\begin{equation}
\tag{4.2}
A = PDP^T
\end{equation}\]</span></p>
<p>Such matrix <span class="math inline">\(A\)</span> is said to be <strong>orthogonally diagonalizable</strong>.</p>
<p>We have seen that for symmetric matrix <span class="math inline">\(A\)</span>, Eq <a href="eigenthings-and-quadratic-forms.html#eq:orthogonal-diagonalization">(4.2)</a> always holds. We can also also verify that if <span class="math inline">\(A\)</span> is orthogonally diagonalizable then it is a symmetric matrix</p>
<p><span class="math display">\[
A^T = (PDP^T)^T = PD^TP^T = PDP^T  = A
\]</span></p>

<div class="theorem">
<span id="thm:unnamed-chunk-4" class="theorem"><strong>Theorem 1.5  </strong></span>An <span class="math inline">\(n \times n\)</span> matrix <span class="math inline">\(A\)</span> is orthogonally diagonalizable if an only if <span class="math inline">\(A\)</span> is a symmetric matrix.
</div>

<div id="spectral-decomposition" class="section level3">
<h3><span class="header-section-number">4.3.1</span> Spectral decomposition</h3>
<p>For orthogonally diagonalizable matrix <span class="math inline">\(A\)</span>, we have</p>
<p><span class="math display">\[
A = PDP^T = [\boldsymbol{q}_1 \;\; \cdots \;\; \boldsymbol{q}_n] 
\begin{bmatrix}
\lambda_1 &amp; &amp; \\
 &amp; \ddots \\
 &amp; &amp; \lambda_n
\end{bmatrix}
\begin{bmatrix}
\boldsymbol{q}_1^T \\
\vdots \\
\boldsymbol{q}_n
\end{bmatrix}
\]</span></p>
<p>It follows that</p>
<p><span class="math display" id="eq:spectral-decomposition">\[\begin{equation}
\tag{4.3}
A = \lambda_1\boldsymbol{q}_1\boldsymbol{q}_1^T + \cdots + \lambda_1\boldsymbol{q}_n\boldsymbol{q}_n^T
\end{equation}\]</span></p>
<p>Eq <a href="eigenthings-and-quadratic-forms.html#eq:spectral-decomposition">(4.3)</a> is called the <strong>spectral decomposition</strong>, breaking <span class="math inline">\(A\)</span> into pieces of rank 1 matrix. It got this name because he set of eigenvalues of a matrix <span class="math inline">\(A\)</span> is sometimes called its <em>spectrum</em>.</p>
</div>
</div>
<div id="svd" class="section level2">
<h2><span class="header-section-number">4.4</span> SVD</h2>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="orthogonality.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="matrix-calculus.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "serif",
"size": 2
},
"edit": {
"link": "https://github.com/enixam/math-foundations/edit/master/eigen-quadratic.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
